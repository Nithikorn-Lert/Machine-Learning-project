{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe content in notebook based on Ref but I try to convert them to Pyspark \\nRef: \\n[1] Advanced Analytics with Spark, 2e\\n[2] https://github.com/novelari/advanced-analytics-spark\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The content in notebook based on Ref but I try to convert them to Pyspark \n",
    "Ref: \n",
    "[1] Advanced Analytics with Spark, 2e\n",
    "[2] https://github.com/novelari/advanced-analytics-spark\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf().setAll([('spark.driver.memory','10g'),('spark.executor.memory', '20g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3')])\n",
    "# config.set(\"spark.driver.memory\", \"10g\")\n",
    "# config.set(\"spark.executer.memory\", \"20g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName= \"Recommendation_sys\", conf = config)\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .master('local')\\\n",
    "                    .appName(\"SparkSession_appName\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = \"C:/Users/New/Downloads/Data science in Action/Data set for spark/advance analytic with spark/data/chapter 3/\"\n",
    "# Spark.SQL_DF\n",
    "# rawUserArtistData_ss = spark.read.option(\"delimiter\",\" \")\\\n",
    "#                                  .option(\"inferSchema\",\"true\")\\\n",
    "#                                  .option(\"charset\", \"utf-8\")\\\n",
    "#                                  .csv(main + \"user_artist_data.txt\").cache()\n",
    "\n",
    "# rawArtistData_ss = spark.read.option(\"delimiter\",'\\t')\\\n",
    "#                           .option(\"inferSchema\",\"true\")\\\n",
    "#                           .option(\"charset\", \"utf-8\")\\\n",
    "#                           .csv(main + \"artist_data.txt\").cache()\n",
    "\n",
    "# rawArtistAlias_ss = spark.read.option(\"delimiter\",'\\t')\\\n",
    "#                           .option(\"inferSchema\",\"true\")\\\n",
    "#                           .option(\"charset\", \"utf-8\")\\\n",
    "#                           .csv(main + \"artist_alias.txt\").cache()\n",
    "\n",
    "# print(type(rawUserArtistData_ss)) # SPARK_SQL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# call data with SparkContext\n",
    "rawUserArtistData = sc.textFile(main + \"user_artist_data.txt\").cache()\n",
    "rawArtistData = sc.textFile(main + \"artist_data.txt\").cache()\n",
    "rawArtistAlias = sc.textFile(main + \"artist_alias.txt\").cache()\n",
    "\n",
    "print(type(rawUserArtistData)) # RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UAD = rawUserArtistData \n",
    "AD = rawArtistData\n",
    "AA = rawArtistAlias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prevent effect of Null value \n",
    "def Int_checker(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Int_checker('a')\n",
    "# => when we use with filter the non-INT data will be remove Automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.writeInt(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:393)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:214)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.writeInt(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:393)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:214)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4424d2f540e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AD'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mAD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.writeInt(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:393)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:214)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.writeInt(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:393)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:214)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\r\n"
     ]
    }
   ],
   "source": [
    "print('AD')\n",
    "AD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_AD = AD.map(lambda each_row: each_row.split('\\t'))\\\n",
    "            .filter(lambda x: x[0] and Int_checker(x[0]))\\\n",
    "            .map(lambda x: (int(x[0]), x[1].strip()))\n",
    "\n",
    "# non-INT data will be remove Automatically\n",
    "# remove all the leading and trailing spaces from a string\n",
    "new_AD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AA')\n",
    "AA.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000311"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Mapping badID to goodID'''\n",
    "new_AA = AA.map(lambda a: a.split('\\t'))\\\n",
    "           .filter(lambda a: a[0] and Int_checker(a[0]))\\\n",
    "           .filter(lambda a: a[1] and Int_checker(a[1]))\\\n",
    "           .map(lambda a: (int(a[0]), int(a[1])))\\\n",
    "           .collectAsMap()\n",
    "\n",
    "# Dict for key = 1092764\n",
    "sc.broadcast(new_AA).value[1092764]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1000002 1 55',\n",
       " '1000002 1000006 33',\n",
       " '1000002 1000007 8',\n",
       " '1000002 1000009 144',\n",
       " '1000002 1000010 314']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('UAD')\n",
    "UAD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''--------------------------------------------------RDD-Based API----------------------------------------------------------'''\n",
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "\n",
    "def buildRatings(rawUserArtistData, bArtistAlias):\n",
    "    def getArtistRating(line):\n",
    "        (userID, artistID, count) = map(lambda x: int(x), line.split(' '))\n",
    "        try:\n",
    "            finalArtistID = bArtistAlias.value[artistID]\n",
    "        except KeyError:\n",
    "            finalArtistID = artistID\n",
    "        return Rating(userID, finalArtistID, count)\n",
    "\n",
    "    return rawUserArtistData.map(lambda line: getArtistRating(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistByID = new_AD\n",
    "artistAlias = new_AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1000002, product=1, rating=55.0),\n",
       " Rating(user=1000002, product=1000006, rating=33.0),\n",
       " Rating(user=1000002, product=1000007, rating=8.0),\n",
       " Rating(user=1000002, product=1000009, rating=144.0),\n",
       " Rating(user=1000002, product=1000010, rating=314.0)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = buildRatings(UAD, bArtistAlias)\n",
    "trainData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ALS.trainImplicit(ratings=trainData, \n",
    "                          rank=50, \n",
    "                          iterations=10, \n",
    "                          lambda_=0.01, \n",
    "                          alpha=40.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.recommendation.MatrixFactorizationModel"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(120,\n",
       "  '0.01847374252974987, 0.1346728354692459, 0.21524611115455627, -0.031568821519613266, 0.05819370225071907, -0.0454770065844059, 0.21174047887325287, 0.06558344513177872, -0.12427325546741486, -0.21571150422096252, 0.14922748506069183, 0.1208685114979744, 0.04389527440071106, 0.013152672909200191, -0.381350576877594, -0.11419039964675903, 0.23068112134933472, 0.2718207538127899, -0.11577887088060379, 0.1131896823644638, -0.28084251284599304, -0.20395854115486145, 0.1309860497713089, 0.021665653213858604, 0.0406518317759037, 0.09064581990242004, 0.0280197374522686, -0.336208313703537, -0.07108686119318008, 0.11044036597013474, -0.21349027752876282, -0.11892475932836533, 0.3537682890892029, 0.07119128853082657, 0.06371480971574783, 0.15184816718101501, -0.06934674084186554, -0.11128142476081848, 0.14201277494430542, -0.03019457682967186, -0.078889861702919, 0.0522034727036953, -0.18735453486442566, 0.11577180027961731, 0.06402997672557831, -0.07317326217889786, 0.04342413693666458, 0.13785462081432343, 0.15241263806819916, -0.09874435514211655'),\n",
       " (384,\n",
       "  '-0.0076913549564778805, 0.46493080258369446, 0.300373375415802, 0.02365332655608654, 0.2072799652814865, 0.15875177085399628, 0.37567824125289917, -0.1896899938583374, -0.20761403441429138, -0.40585625171661377, 0.1217624619603157, -0.2195289134979248, -0.15432322025299072, 0.20788873732089996, -0.2944769263267517, 0.009141255170106888, 0.44805634021759033, 0.3879469335079193, -0.03819828853011131, 0.11322777718305588, -0.6898019313812256, -0.45967724919319153, 0.27474796772003174, -0.10080714523792267, -0.10818277299404144, 0.17351458966732025, -0.1366846114397049, -0.28347206115722656, -0.24044635891914368, 0.19252224266529083, -0.22053399682044983, -0.32747557759284973, 0.10355421155691147, 0.18108324706554413, 0.08574393391609192, 0.18535247445106506, -0.0912109985947609, -0.44987809658050537, -0.0702621266245842, -0.24708837270736694, -0.1293492615222931, 0.22299742698669434, -0.13068784773349762, 0.0012566904770210385, -0.11845703423023224, -0.1511688530445099, 0.1178390234708786, 0.3306736648082733, 0.3286239802837372, -0.005912650376558304'),\n",
       " (828,\n",
       "  '0.0706905797123909, 0.10131300240755081, 0.0401512011885643, 0.06761437654495239, 0.10661902278661728, -0.060441356152296066, 0.18126873672008514, -0.04082934558391571, -0.0868934914469719, -0.11972422152757645, 0.14445367455482483, -0.008972118608653545, -0.08539900183677673, -0.04906429722905159, 0.07287614792585373, -0.021098090335726738, 0.08115717768669128, 0.06179466098546982, -0.04708971455693245, -0.03304162994027138, -0.1514320969581604, -0.13914108276367188, -0.04412036016583443, 0.027630381286144257, 0.012518302537500858, -0.014770647510886192, -0.05628638342022896, -0.23916852474212646, -0.06419145315885544, -0.0007164198905229568, -0.12503103911876678, 0.046583641320466995, 0.16924719512462616, -0.046796586364507675, 0.04270336776971817, 0.05592925846576691, 0.10557157546281815, -0.07059139013290405, -0.07234922796487808, -0.08023830503225327, -0.03522256016731262, 0.09369631111621857, -0.0751631110906601, 0.04907315596938133, -0.03979573771357536, -0.019140424206852913, 0.08269934356212616, 0.01886812224984169, -0.0035381123889237642, -0.0759507492184639'),\n",
       " (3048,\n",
       "  '0.1564646065235138, 0.1538160741329193, 0.39492905139923096, -0.14508286118507385, 0.3023052215576172, -0.2589040696620941, 0.03363427892327309, -0.12631423771381378, -0.4811026155948639, -0.3711267113685608, -0.6343798637390137, -0.00030866338056512177, 0.1733497679233551, -0.29663318395614624, -0.41278278827667236, -0.23116135597229004, -0.05453905463218689, -0.263333261013031, -0.32330283522605896, 0.2907371520996094, -0.7489404082298279, -0.22432200610637665, 0.21355314552783966, -0.12176689505577087, -0.3069595396518707, 0.5779123902320862, 0.3905407786369324, -0.5799217820167542, -0.09093799442052841, 0.4630453586578369, -0.38452082872390747, -0.00897605437785387, 0.6178390383720398, -0.06193919479846954, -0.24590344727039337, -0.007611671928316355, -0.2752978205680847, 0.03736068308353424, 0.07497861236333847, 0.2628585994243622, -0.267050176858902, 0.30981290340423584, -0.20879720151424408, 0.09992876648902893, 0.2451716661453247, -0.1871679127216339, 0.06674478203058243, 0.48524338006973267, 0.3957429528236389, -0.28191861510276794'),\n",
       " (3216,\n",
       "  '0.05398760363459587, 0.24040032923221588, 0.19937001168727875, 0.03869309276342392, 0.2846919000148773, 0.039581768214702606, 0.14958003163337708, 0.1444069892168045, -0.08013497292995453, -0.3930380940437317, 0.14877694845199585, 0.16890621185302734, 0.15381155908107758, 0.09172416478395462, -0.5889772772789001, -0.07221876084804535, 0.3203475773334503, 0.15376998484134674, -0.09072819352149963, 0.44068819284439087, -0.3094044625759125, -0.2542847990989685, 0.4478386342525482, -0.07325226068496704, -0.13279637694358826, 0.1467185616493225, 0.06493329256772995, -0.1583825945854187, -0.03976937755942345, 0.18235474824905396, -0.1874593049287796, -0.28830862045288086, 0.5525978803634644, 0.07126316428184509, -0.03536063805222511, 0.15921109914779663, -0.22929401695728302, -0.07240790128707886, 0.16051320731639862, 0.12030686438083649, -0.013220659457147121, 0.020706618204712868, -0.1974046677350998, 0.13102121651172638, -0.015420537441968918, 0.03141281381249428, 0.1798081248998642, 0.06480627506971359, 0.12716567516326904, 0.11413805931806564')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainData.unpersist()\n",
    "# 50 User Features for 5 user\n",
    "model.userFeatures().mapValues(lambda v: \", \".join( map(lambda x: str(x),v) )).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(user=2093760, product=1001819, rating=0.32046294844618234)\n",
      "Rating(user=2093760, product=1003249, rating=0.32014321506119925)\n",
      "Rating(user=2093760, product=1300642, rating=0.31997051561678613)\n",
      "Rating(user=2093760, product=1007614, rating=0.3199046526287555)\n",
      "Rating(user=2093760, product=829, rating=0.31983416161366623)\n"
     ]
    }
   ],
   "source": [
    "userID = 2093760 # we named this user Sara\n",
    "recommendations = model.recommendProducts(userID, 5)\n",
    "\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_list = [1001819, 1003249, 1300642, 1007614, 892]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1007614, 'Jay-Z'),\n",
       " (1003249, 'Ludacris'),\n",
       " (1001819, '2Pac'),\n",
       " (892, 'Williams Traffic'),\n",
       " (1300642, 'The Game')]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendedArtists = artistByID.filter(lambda artist: artist[0] in recommended_list).collect()\n",
    "\n",
    "recommendedArtists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see what artist are Sara prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawArtistsForUser = rawUserArtistData.map(lambda x: x.split(' '))\\\n",
    "                                     .filter(lambda x: int(x[0]) == userID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1180, 1255340, 378, 813, 942]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existingProducts = rawArtistsForUser.map(lambda x: int(x[1])).collect()\n",
    "existingProducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sara prefers:\n",
      "(1180, 'David Gray')\n",
      "(378, 'Blackalicious')\n",
      "(813, 'Jurassic 5')\n",
      "(1255340, 'The Saw Doctors')\n",
      "(942, 'Xzibit')\n"
     ]
    }
   ],
   "source": [
    "existingArtists = artistByID.filter(lambda artist: artist[0] in existingProducts).collect()\n",
    "print('Sara prefers:')\n",
    "for val in existingArtists:\n",
    "    print(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step we need to analyze what relationship between existingArtists and recommened Artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''----------------------------------------High level API---------------------------------------'''\n",
    "'''Convert RDD(pipeline) to Spark-DF'''\n",
    "from  pyspark.sql.types import Row\n",
    "\n",
    "def f(x):\n",
    "    d = {}\n",
    "    for i in range(len(x)):\n",
    "        d[str(i)] = x[i]\n",
    "    return d\n",
    "\n",
    "# EX\n",
    "# seq = [['a','b','c'],\n",
    "#        ['d','e','f'] ]\n",
    "# print( f(seq) )\n",
    "# print( Row(**f(seq)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCount(rawUserArtistData, bArtistAlias):\n",
    "    \n",
    "    def getArtistCount(line):\n",
    "        (userID, artistID, count) = map(lambda x: int(x), line.split(' '))    \n",
    "        try:\n",
    "            CorrectArtistID = bArtistAlias.value[artistID] # to map bad artist id to good artist id\n",
    "        except KeyError:\n",
    "            CorrectArtistID = artistID\n",
    "        return (userID, CorrectArtistID, count)\n",
    "\n",
    "    return rawUserArtistData.map(lambda line: getArtistCount(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.broadcast use to spread parameter to all excecutor\n",
    "# After we apply sc.broadcast, we can use .value method to see the dict value.\n",
    "bArtistAlias = sc.broadcast(new_AA)\n",
    "# bArtistAlias.value[1092764]\n",
    "trainData_HLV = buildCount(UAD, bArtistAlias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|artist_id|         artist_name|\n",
      "+---------+--------------------+\n",
      "|  1134999|        06Crazy Life|\n",
      "|  6821360|        Pang Nakarin|\n",
      "| 10113088|Terfel, Bartoli- ...|\n",
      "| 10151459| The Flaming Sidebur|\n",
      "|  6826647|   Bodenstandig 3000|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_AD_df = new_AD.map(lambda x: Row(**f(x))).toDF()\n",
    "mapping = dict(zip(['0', '1'], ['artist_id', 'artist_name']))\n",
    "new_AD_df = new_AD_df.select([F.col(c).alias(mapping.get(c)) for c in new_AD_df.columns])\n",
    "\n",
    "new_AD_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|user_id|artist_id|playcount|\n",
      "+-------+---------+---------+\n",
      "|1000002|        1|       55|\n",
      "|1000002|  1000006|       33|\n",
      "|1000002|  1000007|        8|\n",
      "|1000002|  1000009|      144|\n",
      "|1000002|  1000010|      314|\n",
      "+-------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData_df = trainData_HLV.map(lambda x: Row(**f(x))).toDF()\n",
    "mapping = dict(zip(['0', '1', '2'], ['user_id', 'artist_id', 'playcount']))\n",
    "trainData_df = trainData_df.select([F.col(c).alias(mapping.get(c)) for c in trainData_df.columns]) # get 0,1,2 as Keys\n",
    "\n",
    "trainData_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ALS Model in Spark contains the following helpful methods:\n",
    "\n",
    "#### recommendForAllItems(int numUsers)\n",
    "Returns top numUsers users recommended for each item, for all items.\n",
    "\n",
    "#### recommendForAllUsers(int numItems)\n",
    "Returns top numItems items recommended for each user, for all users.\n",
    "\n",
    "#### recommendForItemSubset(Dataset<?> dataset, int numUsers)\n",
    "Returns top numUsers users recommended for each item id in the input data set.\n",
    "\n",
    "#### recommendForUserSubset(Dataset<?> dataset, int numItems)\n",
    "Returns top numItems items recommended for each user id in the input data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID = 2093760\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "model = ALS(implicitPrefs=True,\n",
    "            rank=50,\n",
    "            regParam=0.01, \n",
    "            alpha=40.0,\n",
    "            maxIter=10,\n",
    "            userCol=\"user_id\", \n",
    "            itemCol=\"artist_id\", \n",
    "            ratingCol=\"playcount\")\n",
    "\n",
    "# rank = number of features.\n",
    "\n",
    "# # OR\n",
    "# model = ALS()\n",
    "# (alsEstimator.setRank(10)\n",
    "#   .setUserCol(\"userid\")\n",
    "#   .setItemCol(\"artistid\")\n",
    "#   .setRatingCol(\"playcount\")\n",
    "#   .setMaxIter(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit(trainData_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|user_id|artist_id|playcount|\n",
      "+-------+---------+---------+\n",
      "|2093760|     1180|        1|\n",
      "|2093760|  1255340|        3|\n",
      "|2093760|      378|        1|\n",
      "|2093760|      813|        2|\n",
      "|2093760|      942|        7|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What Sara have been listened\n",
    "trainData_df.filter(trainData_df['user_id'] == userID).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|90 |[-0.053258665, 0.21232685, -0.29962882, -0.10399617, 0.31008646, 0.08170578, -0.34945625, -0.14177836, 0.01756529, 0.14181896, 0.1985244, -0.009506562, 0.27844113, 0.17155527, -0.43065777, -0.26110342, -0.18059972, -0.20134619, 0.10266603, 0.3401965, -0.46696708, 0.025634307, -0.2733108, 0.21599399, 0.26123735, 0.16694577, 0.26603654, -0.12463926, -0.25776857, 0.11994052, -0.106925085, -0.06469849, -0.33023903, -0.46567273, -0.13569142, 0.1386426, -0.010636534, 0.1593971, -0.20422813, 0.35824004, 0.056350548, -0.44322732, -0.06165558, 0.32660255, 0.0037518237, -0.17564213, 0.008859759, -0.09104683, -0.106188364, 0.09246183]|\n",
      "+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_fit.userFactors.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|1000061|\n",
      "|1000070|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "trainData_df.select(model.getUserCol()).distinct().limit(2).show()\n",
    "# model.getUserCol() equivalent to trainData_df['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_df.createOrReplaceTempView('trainData_df_SQL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I prefer this one. It is far better T__T\n",
    "sara = spark.sql('SELECT DISTINCT user_id \\\n",
    "           FROM trainData_df_SQL \\\n",
    "           WHERE user_id = 2093760'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom_user = model_fit.recommendForUserSubset(sara, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------------------+\n",
      "|user_id|recommendations                                                                                                 |\n",
      "+-------+----------------------------------------------------------------------------------------------------------------+\n",
      "|2093760|[[1300642, 0.028670812], [4605, 0.028028714], [829, 0.027974127], [1037970, 0.02780246], [1001819, 0.027772292]]|\n",
      "+-------+----------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recom_user.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
